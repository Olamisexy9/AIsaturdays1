# -*- coding: utf-8 -*-
"""data collection and scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hS1WkV9aXCvAnO2nFH-qYMHY4mj8TTnE
"""

from google.colab import files
files.upload()

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

!kaggle datasets download sahilislam007/college-student-placement-factors-dataset

import zipfile
zip_ref = zipfile.ZipFile('/content/college-student-placement-factors-dataset.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd #import pandas libary

df = pd.read_csv('/content/college_student_placement_dataset.csv') #read csv file
df.head() #view dataset
#print (df)



from google.colab import drive
drive.mount('/content/drive')

"""# **Web Scraping**"""

pip install requests beautifulsoup4

import requests

# Send an HTTP GET request
url = "https://bbc.com"
response = requests.get(url)

# Check the response status code
print("Response Status Code:", response.status_code)

# Print the first 1000 characters of the response content
print("Response Content:")
print(response.text[:1000])

from bs4 import BeautifulSoup

# Parse the HTML content
soup = BeautifulSoup(response.content, "html.parser")

# Print the prettified HTML
print(soup.prettify())

# Find the first <h1> tag and extract its text
header = soup.find("h2")
print("Header:", header)

# Find all <a> tags and extract their href attributes
links = soup.find_all("a")
for link in links:
    print("Link:", link.get("href"))

import os

url = "https://www.bbc.com/"

response = requests.get(url)

if response.status_code == 200:
    soup = BeautifulSoup(response.content, 'html.parser')

    # Scrape images
    image_folder = 'images'
    os.makedirs(image_folder, exist_ok=True)
    image_tags = soup.find_all('img')

    for img_tag in image_tags:
        img_url = img_tag['src']
        if not img_url.startswith('http'):
            img_url = url + img_url
        img_response = requests.get(img_url)

        # Save the image to the images folder
        with open(os.path.join(image_folder, os.path.basename(img_url)), 'wb') as img_file:
            img_file.write(img_response.content)

    # Scrape text
    text_data = []
    text_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div'])

    for tag in text_tags:
        text_data.append(tag.get_text())

    # Print the text data
    print("\n".join(text_data))

else:
    print(f"Failed to fetch data from {url}. Status code: {response.status_code}")